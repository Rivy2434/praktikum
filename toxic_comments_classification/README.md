# Classification of toxic comments 
The aim of this project was to create a model which can distinguish toxic and non-toxic comments with minimum F1 score of 0.75.
Labeled data contains text comment in <i>text</i>, and target in <i>toxic</i>.
For feature engeneering TF-IDF was used.
Following libraries were used:
pandas, nltk, sklearn, matplotlib, lightgbm, torch, spacy

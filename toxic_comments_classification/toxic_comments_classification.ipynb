{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comments classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">For pre-moderation of comments there is a need in automated text classification. The aim is to detect toxic comments and queueing them for moderation. <br>\n",
    "Using available text data one should train model to distinguish toxic and non-toxic comments. \n",
    "Labeled data are following: <i>text</i> contains text comment, and toxic is a target. <br> Desirable F1 score is at least 0.75. </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import re\n",
    "from tqdm import notebook\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, SCORERS\n",
    "from sklearn.naive_bayes import MultinomialNB,ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to folder with stopwords and data\n",
    "path = 'D:/Bot/DataScience/Jupyter projects/3 module/project 4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loading data\n",
    "try:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv') #переводим даты в даты и делаем их индексами\n",
    "    print('Using remote files')\n",
    "    print()\n",
    "except:\n",
    "    data = pd.read_csv(path + '/toxic_comments.csv')\n",
    "    print('Using local files')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n",
      "Доля классов в общей выборке: 0    0.898321\n",
      "1    0.101679\n",
      "Name: toxic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#data overview\n",
    "display(data.head())\n",
    "print(data.info())\n",
    "#checking target for class imbalance\n",
    "print('Доля классов в общей выборке:',data['toxic'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation and other symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removeing everything but letters and spaces\n",
    "#and converting them to lowercase\n",
    "def purge(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z]',' ',text)\n",
    "    \n",
    "    text.split()\n",
    "    \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 659 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#using spacy for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "#function for sentence lemmatization\n",
    "def lemm(text):\n",
    "    value = nlp(text)\n",
    "    lemmatized = \" \".join([token.lemma_ for token in value])\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#removing symbols and converting to lowercase using function\n",
    "data['purged'] = data['text'].apply(purge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#lemmatizing\n",
    "data['purged'] = data['purged'].apply(lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>purged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edit make under my usernam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww   he match this background colour I m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man   I m really not try to edit war   it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more I can t make any real suggestion on im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you   sir   be my hero   any chance you rememb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                              purged  \n",
       "0  explanation why the edit make under my usernam...  \n",
       "1  d aww   he match this background colour I m se...  \n",
       "2  hey man   I m really not try to edit war   it ...  \n",
       "3     more I can t make any real suggestion on im...  \n",
       "4  you   sir   be my hero   any chance you rememb...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checking out\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords need to be removed, we make the list for them. One is from nltk library, and browse for additional in the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also read</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>read also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>read more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&amp;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'ll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>youre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>yours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>yourself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>yourselves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>730 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      also read\n",
       "0     read also\n",
       "1     read more\n",
       "2        source\n",
       "3             &\n",
       "4           'll\n",
       "..          ...\n",
       "725       youre\n",
       "726       yours\n",
       "727    yourself\n",
       "728  yourselves\n",
       "729        zero\n",
       "\n",
       "[730 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://github.com/saurabbhsp/stopwords source for stopwords\n",
    "\n",
    "additonal_words = pd.read_csv(path + '/additional_words.csv')\n",
    "display(additonal_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anastasia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "#merging lists\n",
    "stopwords.words('english').append(additonal_words.values)\n",
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for stopwords removing \n",
    "def del_stop_words(text):\n",
    "    text = text.split()\n",
    "    new_text = []\n",
    "    for element in text:\n",
    "        if element not in stop_words:\n",
    "            new_text.append(element)\n",
    "    new_text = \" \".join(new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['purged'] = data['purged'].apply(del_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>purged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation edit make username hardcore metall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>aww match background colour I seemingly stick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man I really try edit war guy constantly r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>I make real suggestion improvement I wonder se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                              purged  \n",
       "0  explanation edit make username hardcore metall...  \n",
       "1  aww match background colour I seemingly stick ...  \n",
       "2  hey man I really try edit war guy constantly r...  \n",
       "3  I make real suggestion improvement I wonder se...  \n",
       "4                      sir hero chance remember page  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656,) (127656,)\n",
      "(31915,) (31915,)\n",
      "Wall time: 32.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features = data['purged']\n",
    "target = data['toxic']\n",
    "\n",
    "features_train, features_test, target_train,  target_test = train_test_split(features,target, test_size = 0.2,random_state = 12345)\n",
    "#checking subsets size\n",
    "print(features_train.shape, target_train.shape)\n",
    "print(features_test.shape,target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project it was decided to use TfidfVectorizer for feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#using Tfidf for feature engeneering\n",
    "train_corpus = list(features_train)\n",
    "#making sets of unigramms\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words,ngram_range=(1, 1))\n",
    "count_tf_idf.fit(train_corpus)\n",
    "\n",
    "x_train = count_tf_idf.transform(train_corpus)\n",
    "train_dic = count_tf_idf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = list(features_test)\n",
    "x_test = count_tf_idf.transform(test_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sets with uni- and bigramms for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#and additional set with bigramms\n",
    "counter = TfidfVectorizer(stop_words=stop_words,ngram_range=(1, 2))\n",
    "counter.fit(train_corpus)\n",
    "x2_train = counter.transform(train_corpus)\n",
    "\n",
    "x2_test = counter.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessory functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary for models and counter\n",
    "models_dict = {}\n",
    "\n",
    "predictions_train_dict = {}\n",
    "predictions_test_dict = {}\n",
    "\n",
    "f1_train_dict = {}\n",
    "f1_test_dict = {}\n",
    "\n",
    "Counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656, 132665) (127656,) (31915, 132665) (31915,)\n",
      "(127656, 2065784) (127656,) (31915, 2065784) (31915,)\n"
     ]
    }
   ],
   "source": [
    "#features lists\n",
    "features_list = [x_train,target_train,x_test,target_test]\n",
    "print(x_train.shape,target_train.shape,x_test.shape,target_test.shape)\n",
    "\n",
    "features_list2 = [x2_train,target_train,x2_test,target_test]\n",
    "print(x2_train.shape,target_train.shape,x2_test.shape,target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for model training and F1 printing\n",
    "\n",
    "def train_model(model,features_list):\n",
    "    \n",
    "    global Counter\n",
    "    Counter += 1 #counter counts every function call\n",
    "    \n",
    "    name = 'model'+str(Counter) \n",
    "    \n",
    "    predictions_train = 'predictions_train' + '_' + name\n",
    "    predictions_test = 'predictions_test' + '_' + name\n",
    "            \n",
    "    f1_train = 'f1_train' + '_' + name\n",
    "    f1_test = 'f1_test' + '_' + name\n",
    "    \n",
    "    models_dict[name] = model\n",
    "    models_dict[name].fit(features_list[0],features_list[1])\n",
    "    \n",
    "    #features_list = [features_train,target_train,features_test,target_test] just to remember how it is looks like\n",
    "    \n",
    "    predictions_train_dict[predictions_train] = models_dict[name].predict(features_list[0])\n",
    "    predictions_test_dict[predictions_test] = models_dict[name].predict(features_list[2])\n",
    "    \n",
    "    f1_train_dict[f1_train] = f1_score(features_list[1],predictions_train_dict[predictions_train])\n",
    "    f1_test_dict[f1_test] = f1_score(features_list[3],predictions_test_dict[predictions_test])\n",
    "    \n",
    "    print('train F1:', f1_train_dict[f1_train])\n",
    "    print('test F1:',f1_test_dict[f1_test])\n",
    "    return models_dict[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding pipeline for cross-validation (count_tf_idf - unigramms, counter - uni- and bigramms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new features lists for pipelines\n",
    "features_list_ = [features_train,target_train,features_test,target_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(model,tf_vectorizer):\n",
    "    pipe = Pipeline([('TF-IDF',tf_vectorizer),('model',model)])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function, which is\n",
    "#taking models name and dictionary with parameters for GridSearch\n",
    "#making pipelines for model and uni- and bigramms\n",
    "#all paramers X for model should be like model__X in the dictionary\n",
    "def lasy(model,parameters):\n",
    "    #pipelines\n",
    "    model_pipe_uni = pipe(model,count_tf_idf)\n",
    "    model_pipe_bi = pipe(model,counter)\n",
    "    \n",
    "    gsearch_model_uni = GridSearchCV(model_pipe_uni,parameters,scoring='f1',n_jobs=-1,cv=5)\n",
    "    gsearch_model_uni.fit(features_train,target_train)\n",
    "    print('parameter for model with unigramms:',gsearch_model_uni.best_params_)\n",
    "    \n",
    "    gsearch_model_bi = GridSearchCV(model_pipe_bi,parameters,scoring='f1',n_jobs=-1,cv=5)\n",
    "    gsearch_model_bi.fit(features_train,target_train)\n",
    "    print('parameter for model with bigramms:',gsearch_model_bi.best_params_)\n",
    "    \n",
    "    #проверяем на трейне и тесте\n",
    "    print('Model trained on unigramms:')\n",
    "    model_uni = train_model(gsearch_model_uni.best_estimator_,features_list_)\n",
    "    \n",
    "    print('Model trained on uni- and bigramms:')\n",
    "    model_bi = train_model(gsearch_model_bi.best_estimator_,features_list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter for model with unigramms: {'model__C': 10.0}\n",
      "parameter for model with bigramms: {'model__C': 10.0}\n",
      "Wall time: 6min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "linear_model = LogisticRegression(class_weight='balanced',solver='lbfgs',max_iter=400)\n",
    "#using pipeline\n",
    "linear_pipe_uni = pipe(linear_model,count_tf_idf)\n",
    "linear_pipe_bi = pipe(linear_model,counter)\n",
    "#varying C\n",
    "parameters_linear = {\n",
    "    'model__C':[0.1,10.]\n",
    "    \n",
    "}\n",
    "gsearch_linear_uni = GridSearchCV(linear_pipe_uni,parameters_linear,scoring='f1',n_jobs=-1,cv=5)\n",
    "gsearch_linear_uni.fit(features_train,target_train)\n",
    "print('parameter for model with unigramms:',gsearch_linear_uni.best_params_)\n",
    "\n",
    "gsearch_linear_bi = GridSearchCV(linear_pipe_bi,parameters_linear,scoring='f1',n_jobs=-1,cv=5)\n",
    "gsearch_linear_bi.fit(features_train,target_train)\n",
    "print('parameter for model with bigramms:',gsearch_linear_bi.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained on unigramms:\n",
      "train F1: 0.9070380922232775\n",
      "test F1: 0.7610795454545454\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Model trained on unigramms:')\n",
    "linear_model3 = train_model(gsearch_linear_uni.best_estimator_,features_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained on bigramms:\n",
      "train F1: 0.9831869130566495\n",
      "test F1: 0.7900452488687781\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Model trained on bigramms:')\n",
    "linear_model4 = train_model(gsearch_linear_bi.best_estimator_,features_list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_tree = {\n",
    "    'model__criterion':('gini', 'entropy'),\n",
    "    'model__splitter':('best','random'),\n",
    "    'model__max_depth':[2,20],\n",
    "    'model__max_features':('auto','sqrt','log2'),   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter for model with unigramms: {'model__criterion': 'entropy', 'model__max_depth': 20, 'model__max_features': 'auto', 'model__splitter': 'best'}\n",
      "parameter for model with bigramms: {'model__criterion': 'gini', 'model__max_depth': 20, 'model__max_features': 'auto', 'model__splitter': 'best'}\n",
      "Model trained on unigramms:\n",
      "train F1: 0.21569454036648697\n",
      "test F1: 0.2133170257922947\n",
      "Model trained on uni- and bigramms:\n",
      "train F1: 0.20031522675202704\n",
      "test F1: 0.19801466294686304\n",
      "Wall time: 13min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree_model = DecisionTreeClassifier(random_state=12345,class_weight='balanced')\n",
    "lasy(tree_model,parameters_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_forest = {\n",
    "    'model__n_estimators':[10,100],\n",
    "    'model__max_depth':[2,15],\n",
    "}\n",
    "forest_model = RandomForestClassifier(random_state=12345,class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter for model with unigramms: {'model__max_depth': 15, 'model__n_estimators': 100}\n",
      "parameter for model with bigramms: {'model__max_depth': 15, 'model__n_estimators': 100}\n",
      "Model trained on unigramms:\n",
      "train F1: 0.37193970621179634\n",
      "test F1: 0.35501216233516836\n",
      "Model trained on uni- and bigramms:\n",
      "train F1: 0.3616288151426515\n",
      "test F1: 0.34536334719843403\n",
      "Wall time: 5min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lasy(forest_model,parameters_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM = LGBMClassifier(random_state = 12345,class_weight='balanced',n_estimators=100,n_jobs=-1)\n",
    "parameters_LGBMC = {\n",
    "    'model__boosting_type':('gbdt','dart','goss'),\n",
    "    'model__max_depth':[10,40],\n",
    "    'model__num_leaves':[10,30]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "параметр для модели на униграммах: {'model__boosting_type': 'gbdt', 'model__max_depth': 40, 'model__num_leaves': 30}\n",
      "параметр для модели на биграммах: {'model__boosting_type': 'gbdt', 'model__max_depth': 40, 'model__num_leaves': 30}\n",
      "Wall time: 25min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#here one can try to reduce the number of cross-val sets and parameters\n",
    "LGBM_pipe_uni = pipe(LGBM,count_tf_idf)\n",
    "LGBM_pipe_bi = pipe(LGBM,counter)\n",
    "    \n",
    "gsearch_LGBM_uni = GridSearchCV(LGBM_pipe_uni,parameters_LGBMC,scoring='f1',n_jobs=-1,cv=3)\n",
    "gsearch_LGBM_uni.fit(features_train,target_train)\n",
    "print('параметр для модели на униграммах:',gsearch_LGBM_uni.best_params_)\n",
    "    \n",
    "gsearch_LGBM_bi = GridSearchCV(LGBM_pipe_bi,parameters_LGBMC,scoring='f1',n_jobs=-1,cv=3)\n",
    "gsearch_LGBM_bi.fit(features_train,target_train)\n",
    "print('параметр для модели на биграммах:',gsearch_LGBM_bi.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель, обученная на униграммах:\n",
      "train F1: 0.772264851914864\n",
      "test F1: 0.7373439910251017\n",
      "Wall time: 37.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Модель, обученная на униграммах:')\n",
    "LGBM_uni = train_model(gsearch_LGBM_uni.best_estimator_,features_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель, обученная на униграммах и биграммах:\n",
      "train F1: 0.7741755948239877\n",
      "test F1: 0.7297520661157024\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Модель, обученная на униграммах и биграммах:')\n",
    "LGBM_bi = train_model(gsearch_LGBM_bi.best_estimator_,features_list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = MultinomialNB()\n",
    "parameters_NB = {\n",
    "    'model__alpha':[1.0e-10,10],\n",
    "    'model__fit_prior':(True,False)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter for model with unigramms: {'model__alpha': 1e-10, 'model__fit_prior': True}\n",
      "parameter for model with bigramms: {'model__alpha': 1e-10, 'model__fit_prior': True}\n",
      "Model trained on unigramms:\n",
      "train F1: 0.8684188428919305\n",
      "test F1: 0.6186473060756592\n",
      "Model trained on uni- and bigramms:\n",
      "train F1: 0.9900649598777226\n",
      "test F1: 0.5570376269161855\n",
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lasy(NB_model,parameters_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNB_model = ComplementNB()\n",
    "parameters_CNB = {\n",
    "    'model__alpha':[1.0e-10,10],\n",
    "    'model__fit_prior':(True,False),\n",
    "    'model__norm':(True,False)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter for model with unigramms: {'model__alpha': 1e-10, 'model__fit_prior': True, 'model__norm': False}\n",
      "parameter for model with bigramms: {'model__alpha': 1e-10, 'model__fit_prior': True, 'model__norm': False}\n",
      "Model trained on unigramms:\n",
      "train F1: 0.7699015213878472\n",
      "test F1: 0.5506663314055821\n",
      "Model trained on uni- and bigramms:\n",
      "train F1: 0.9629299788489368\n",
      "test F1: 0.5316191799861014\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lasy(CNB_model,parameters_CNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data were uploaded and inspected\n",
    " - data have notable imbalance of classes\n",
    "- Text comments were lowercased, purified and lemmatized\n",
    "- Stopwords were removed\n",
    "- Data were split into train and test sets in a 1 to 5 ration\n",
    "- TF-IDF values of words were used as features\n",
    "- Several models were trained and tested\n",
    " - For some of them parameters were chosen by the means of GridSearchCV \n",
    " - The best F1 score on test (0.79) was achieved for linear regression model trained on uni- and bigramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1280,
    "start_time": "2021-10-17T18:47:54.368Z"
   },
   {
    "duration": 614,
    "start_time": "2021-10-17T18:47:55.650Z"
   },
   {
    "duration": 36,
    "start_time": "2021-10-17T18:47:56.267Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-17T18:47:56.304Z"
   },
   {
    "duration": 95506,
    "start_time": "2021-10-17T18:47:56.310Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-17T18:49:31.818Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-17T18:50:09.310Z"
   },
   {
    "duration": 355,
    "start_time": "2021-10-17T19:22:58.543Z"
   },
   {
    "duration": 85,
    "start_time": "2021-10-17T19:55:26.419Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-17T19:55:49.167Z"
   },
   {
    "duration": 238,
    "start_time": "2021-10-17T19:57:37.433Z"
   },
   {
    "duration": 88,
    "start_time": "2021-10-17T19:57:58.547Z"
   },
   {
    "duration": 258,
    "start_time": "2021-10-17T19:58:01.858Z"
   },
   {
    "duration": 241,
    "start_time": "2021-10-17T19:58:09.023Z"
   },
   {
    "duration": 245,
    "start_time": "2021-10-17T19:58:13.548Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-17T21:41:40.330Z"
   },
   {
    "duration": 735,
    "start_time": "2021-10-17T21:41:40.339Z"
   },
   {
    "duration": 35,
    "start_time": "2021-10-17T21:41:41.076Z"
   },
   {
    "duration": 257,
    "start_time": "2021-10-17T21:41:41.113Z"
   },
   {
    "duration": -894,
    "start_time": "2021-10-17T21:41:42.266Z"
   },
   {
    "duration": -894,
    "start_time": "2021-10-17T21:41:42.267Z"
   },
   {
    "duration": -895,
    "start_time": "2021-10-17T21:41:42.269Z"
   },
   {
    "duration": 2401,
    "start_time": "2021-10-17T21:58:55.936Z"
   },
   {
    "duration": 1254,
    "start_time": "2021-10-17T22:00:37.722Z"
   },
   {
    "duration": 678,
    "start_time": "2021-10-17T22:00:43.965Z"
   },
   {
    "duration": 43,
    "start_time": "2021-10-17T22:00:45.799Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-17T22:01:39.578Z"
   },
   {
    "duration": 364,
    "start_time": "2021-10-17T22:02:41.438Z"
   },
   {
    "duration": 261,
    "start_time": "2021-10-17T22:02:52.995Z"
   },
   {
    "duration": 81,
    "start_time": "2021-10-17T22:03:56.405Z"
   },
   {
    "duration": 258,
    "start_time": "2021-10-17T22:04:00.615Z"
   },
   {
    "duration": 248,
    "start_time": "2021-10-17T22:04:06.022Z"
   },
   {
    "duration": 1781,
    "start_time": "2021-10-17T22:04:18.726Z"
   },
   {
    "duration": 14,
    "start_time": "2021-10-17T22:04:40.509Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-17T22:05:39.181Z"
   },
   {
    "duration": 1769,
    "start_time": "2021-10-17T22:05:40.021Z"
   },
   {
    "duration": 2122,
    "start_time": "2021-10-17T22:05:53.041Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-17T22:05:55.166Z"
   },
   {
    "duration": 697,
    "start_time": "2021-10-17T22:05:55.170Z"
   },
   {
    "duration": 36,
    "start_time": "2021-10-17T22:05:55.869Z"
   },
   {
    "duration": 2,
    "start_time": "2021-10-17T22:05:55.907Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-17T22:06:02.042Z"
   },
   {
    "duration": 1240,
    "start_time": "2021-10-17T22:06:04.048Z"
   },
   {
    "duration": 215,
    "start_time": "2021-10-17T22:37:22.265Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-17T22:37:56.133Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-17T22:42:33.503Z"
   },
   {
    "duration": 241,
    "start_time": "2021-10-17T22:43:21.736Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-17T22:43:25.865Z"
   },
   {
    "duration": 1399,
    "start_time": "2021-10-17T22:43:26.876Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-17T22:43:46.033Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-17T22:44:06.741Z"
   },
   {
    "duration": 121894,
    "start_time": "2021-10-17T22:44:22.745Z"
   },
   {
    "duration": 11,
    "start_time": "2021-10-17T22:46:35.148Z"
   },
   {
    "duration": 81,
    "start_time": "2021-10-17T22:51:24.064Z"
   },
   {
    "duration": 86,
    "start_time": "2021-10-17T22:51:34.294Z"
   },
   {
    "duration": 2225,
    "start_time": "2021-10-17T22:51:52.447Z"
   },
   {
    "duration": 4241,
    "start_time": "2021-10-17T22:52:07.171Z"
   },
   {
    "duration": 409,
    "start_time": "2021-10-17T22:52:32.882Z"
   },
   {
    "duration": 702,
    "start_time": "2021-10-17T22:54:44.376Z"
   },
   {
    "duration": 709,
    "start_time": "2021-10-17T22:55:47.125Z"
   },
   {
    "duration": 11,
    "start_time": "2021-10-17T22:55:50.677Z"
   },
   {
    "duration": 699820,
    "start_time": "2021-10-17T22:55:58.703Z"
   },
   {
    "duration": 12,
    "start_time": "2021-10-17T23:07:38.526Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-17T23:14:09.827Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-17T23:35:25.168Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-17T23:36:13.144Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-17T23:36:17.767Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-17T23:36:30.580Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-17T23:36:31.171Z"
   },
   {
    "duration": 9,
    "start_time": "2021-10-17T23:37:34.855Z"
   },
   {
    "duration": 6090,
    "start_time": "2021-10-17T23:37:38.512Z"
   },
   {
    "duration": 10,
    "start_time": "2021-10-17T23:37:44.604Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "457px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
